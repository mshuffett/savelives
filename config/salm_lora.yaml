# NeMo SALM LoRA Fine-Tuning Configuration
# Based on nvidia/canary-qwen-2.5b training guidelines

name: canary_qwen_medical_lora

trainer:
  devices: 1
  accelerator: gpu
  num_nodes: 1
  precision: bf16  # Use bfloat16 for A100
  max_epochs: 5
  max_steps: -1  # -1 for epoch-based training
  val_check_interval: 500
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4

  logger:
    - class_path: pytorch_lightning.loggers.WandbLogger
      init_args:
        project: medical-asr-poc-michael
        name: canary-qwen-medical-lora-v1
        save_dir: /workspace/logs/wandb
        log_model: false  # Don't upload full model to wandb

  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        dirpath: /workspace/checkpoints/canary-qwen-medical-lora
        filename: '{epoch}-{step}-{val_wer:.4f}'
        monitor: val_wer
        mode: min
        save_top_k: 3
        save_last: true
        every_n_train_steps: 500

model:
  # Base model configuration
  restore_from_path: nvidia/canary-qwen-2.5b  # HuggingFace model ID

  # Freeze settings (per NeMo guidelines)
  freeze_llm: true  # Freeze LLM weights
  freeze_audio_encoder: false  # Keep encoder trainable

  # LoRA/PEFT Configuration
  peft:
    peft_scheme: lora
    adapter_dim: 16  # LoRA rank (r)
    alpha: 32  # LoRA alpha (typically 2x rank)
    dropout: 0.1
    adapter_dropout: 0.1
    target_modules:
      - linear_qkv
      - linear_fc1
      - linear_fc2
      - linear_proj

  # Training data
  train_ds:
    manifest_filepath: /workspace/data/processed/train_manifest.json
    sample_rate: 16000
    batch_size: 4  # Adjust based on GPU memory
    shuffle: true
    num_workers: 4
    pin_memory: true
    max_duration: 40.0  # Max 40 seconds per NeMo guidelines
    min_duration: 0.5
    trim_silence: true

  # Validation data
  validation_ds:
    manifest_filepath: /workspace/data/processed/val_manifest.json
    sample_rate: 16000
    batch_size: 4
    shuffle: false
    num_workers: 4
    max_duration: 40.0
    min_duration: 0.5

  # Optimizer configuration
  optim:
    name: adamw
    lr: 2e-4  # Learning rate for LoRA
    betas: [0.9, 0.999]
    weight_decay: 0.01

    sched:
      name: CosineAnnealing
      warmup_steps: 100
      min_lr: 1e-6

# Experiment metadata
exp_manager:
  name: canary_qwen_medical_lora
  exp_dir: /workspace/logs/nemo_experiments
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  checkpoint_callback_params:
    monitor: val_wer
    mode: min
    save_top_k: 3
  resume_if_exists: true
  resume_ignore_no_checkpoint: true
