# NeMo SALM LoRA Fine-Tuning Configuration
# Based on nvidia/canary-qwen-2.5b training guidelines

name: canary_qwen_medical_lora

trainer:
  devices: 1
  accelerator: gpu
  num_nodes: 1
  precision: bf16  # Use bfloat16 for A100
  max_epochs: 5
  max_steps: -1  # -1 for epoch-based training
  val_check_interval: 500
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4

  logger:
    - class_path: pytorch_lightning.loggers.WandbLogger
      init_args:
        project: medical-asr-poc-michael
        name: canary-qwen-medical-lora-v1
        save_dir: /workspace/logs/wandb
        log_model: false  # Don't upload full model to wandb

  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        dirpath: /workspace/checkpoints/canary-qwen-medical-lora
        filename: '{epoch}-{step}-{val_wer:.4f}'
        monitor: val_wer
        mode: min
        save_top_k: 3
        save_last: true
        every_n_train_steps: 500

model:
  # Use the published SALM checkpoint as base
  restore_from_path: nvidia/canary-qwen-2.5b

  # Optional LoRA configuration (delegated to NeMo SpeechLM2)
  lora:
    task_type: CAUSAL_LM
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1

  # Optional freeze policy
  # Many SALM examples freeze substantial portions of the LLM; rely on NeMo defaults

data:
  train_ds:
    manifest_filepath: /workspace/data/processed/train_manifest.json
    sample_rate: 16000
    batch_size: 4
    shuffle: true
    num_workers: 4
    pin_memory: true
    max_duration: 40.0
    min_duration: 0.5
  validation_ds:
    manifest_filepath: /workspace/data/processed/val_manifest.json
    sample_rate: 16000
    batch_size: 4
    shuffle: false
    num_workers: 4
    max_duration: 40.0
    min_duration: 0.5

optim:
  name: adamw
  lr: 2e-4
  betas: [0.9, 0.999]
  weight_decay: 0.01
  sched:
    name: CosineAnnealing
    warmup_steps: 100
    min_lr: 1e-6

# Experiment metadata
exp_manager:
  name: canary_qwen_medical_lora
  exp_dir: /workspace/logs/nemo_experiments
  create_tensorboard_logger: true
  create_wandb_logger: true
  wandb_logger_kwargs:
    project: medical-asr-poc-michael
    name: canary-qwen-medical-lora-v1
    save_dir: /workspace/logs/wandb
    log_model: false
  create_checkpoint_callback: true
  checkpoint_callback_params:
    monitor: val_wer
    mode: min
    save_top_k: 3
  resume_if_exists: true
  resume_ignore_no_checkpoint: true
